{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Packages Used"
      ],
      "metadata": {
        "id": "sney7A6hER7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import gdown\n",
        "import os\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import io\n",
        "from google.colab import drive\n",
        "import time"
      ],
      "metadata": {
        "id": "FQY_WaOVERW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: Loading a Large CSV"
      ],
      "metadata": {
        "id": "m1CTphcSulIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file name and path\n",
        "local_file_path = '/content/drive/MyDrive/BDA_Datasets/activity_log_raw.csv'  # Path to the input CSV file\n",
        "\n",
        "def process_chunk(chunk, date_format):\n",
        "    \"\"\"\n",
        "    Processes each chunk by removing null characters and performing data cleaning.\n",
        "\n",
        "    Args:\n",
        "        chunk (DataFrame): A chunk of the data to be processed.\n",
        "        date_format (str): The date format to use when converting to datetime.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: The cleaned and processed chunk.\n",
        "    \"\"\"\n",
        "    # Replace NUL characters with empty strings in the ACTIVITY_TIME column\n",
        "    if 'ACTIVITY_TIME' in chunk.columns:\n",
        "        chunk['ACTIVITY_TIME'] = chunk['ACTIVITY_TIME'].str.replace('\\x00', '', regex=False)\n",
        "\n",
        "        # Remove rows with missing activity times and duplicate rows\n",
        "        chunk.dropna(subset=['ACTIVITY_TIME'], inplace=True)\n",
        "        chunk.drop_duplicates(inplace=True)\n",
        "\n",
        "        # Convert ACTIVITY_TIME to datetime format and drop invalid rows\n",
        "        chunk['ACTIVITY_TIME'] = pd.to_datetime(chunk['ACTIVITY_TIME'], format=date_format, errors='coerce')\n",
        "        chunk.dropna(subset=['ACTIVITY_TIME'], inplace=True)\n",
        "    return chunk\n",
        "\n",
        "def process_all_chunks(file_path, chunksize=100000, date_format='%d-%b-%y %I.%M.%S.%f %p', encoding='latin-1'):\n",
        "    \"\"\"\n",
        "    Processes all chunks of the CSV file and calculates the date range and busiest year/month.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the CSV file.\n",
        "        chunksize (int): Number of rows to read per chunk.\n",
        "        date_format (str): Format of the datetime column.\n",
        "        encoding (str): Encoding of the CSV file.\n",
        "\n",
        "    Returns:Financial Mathematics\n",
        "        tuple: The date range (min_date, max_date) and busiest year/month.\n",
        "    \"\"\"\n",
        "    if not file_path:\n",
        "        print(\"File path is invalid.\")\n",
        "        return None, None\n",
        "\n",
        "    # Initialize variables for date range and year/month counts\n",
        "    min_date = None\n",
        "    max_date = None\n",
        "    all_year_month_counts = pd.Series(dtype='int64')  # To store year/month counts across chunks\n",
        "\n",
        "    try:\n",
        "        print(f\"Processing CSV file in chunks with encoding: {encoding}...\")\n",
        "\n",
        "        # Read the file in chunks\n",
        "        chunk_iterator = pd.read_csv(\n",
        "            file_path,\n",
        "            chunksize=chunksize,\n",
        "            encoding=encoding,\n",
        "            sep=',',\n",
        "            on_bad_lines='skip',  # Skip bad lines in the file\n",
        "            engine='python',\n",
        "            iterator=True,\n",
        "            usecols=['ACTIVITY_TIME']  # Read only the ACTIVITY_TIME column\n",
        "        )\n",
        "\n",
        "        processed_chunks = 0\n",
        "        for chunk in chunk_iterator:\n",
        "            processed_chunks += 1\n",
        "\n",
        "            # Skip empty chunks\n",
        "            if chunk is None or chunk.empty:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Process each chunk using the process_chunk function\n",
        "                chunk = process_chunk(chunk, date_format)\n",
        "            except Exception as e:\n",
        "                continue  # Skip chunks with processing errors\n",
        "\n",
        "            # Skip empty chunks after processing\n",
        "            if chunk.empty:\n",
        "                continue\n",
        "\n",
        "            # Update the min and max dates\n",
        "            current_min = chunk['ACTIVITY_TIME'].min()\n",
        "            current_max = chunk['ACTIVITY_TIME'].max()\n",
        "\n",
        "            if min_date is None or current_min < min_date:\n",
        "                min_date = current_min\n",
        "            if max_date is None or current_max > max_date:\n",
        "                max_date = current_max\n",
        "\n",
        "            # Calculate year/month counts only if ACTIVITY_TIME column is valid\n",
        "            if 'ACTIVITY_TIME' in chunk.columns and not chunk['ACTIVITY_TIME'].empty:\n",
        "                chunk['year_month'] = chunk['ACTIVITY_TIME'].dt.to_period('M')\n",
        "                year_month_counts = chunk['year_month'].value_counts()\n",
        "                all_year_month_counts = all_year_month_counts.add(year_month_counts, fill_value=0)\n",
        "            else:\n",
        "                print(\"No valid ACTIVITY_TIME column to calculate year_month counts\")\n",
        "\n",
        "        # Check if any valid data was found\n",
        "        if min_date is not None and max_date is not None:\n",
        "            print(\"Finished processing all chunks.\")\n",
        "            if not all_year_month_counts.empty:\n",
        "                busiest_year_month = all_year_month_counts.idxmax()\n",
        "                return (min_date, max_date), (busiest_year_month.year, busiest_year_month.month)\n",
        "            else:\n",
        "                return (min_date, max_date), None\n",
        "        else:\n",
        "            print(\"No valid data found.\")\n",
        "            return None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during overall processing: {e}\")\n",
        "        # Return results based on available data if an error occurs\n",
        "        if min_date is not None and max_date is not None and not all_year_month_counts.empty:\n",
        "            busiest_year_month = all_year_month_counts.idxmax()\n",
        "            return (min_date, max_date), (busiest_year_month.year, busiest_year_month.month)\n",
        "        else:\n",
        "            return (min_date, max_date), None\n",
        "\n",
        "# Inform the user about the estimated running time\n",
        "print(\"Estimated maximum running time: approximately 11 minutes.\")\n",
        "print(\"Please wait...\")\n",
        "\n",
        "# Process the file and calculate the date range and busiest year/month\n",
        "date_range, busiest_year_month = process_all_chunks(local_file_path, chunksize=100000)\n",
        "print(\"ignore Error message\")\n"
      ],
      "metadata": {
        "id": "1FqDlZ4RsEMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f24e1ddf-c48d-4583-bb92-a34bc3db620d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated maximum running time: approximately 11 minutes.\n",
            "Please wait...\n",
            "Processing CSV file in chunks with encoding: latin-1...\n",
            "Error during overall processing: line contains NUL\n",
            "ignore Error message\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task-1.\n",
        "\n",
        "#### Write a function which returns the date range in the dataset"
      ],
      "metadata": {
        "id": "p5mchTLwsPGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if date_range:\n",
        "    if date_range[0] != None and date_range[1] != None:\n",
        "      earliest_date, latest_date = date_range\n",
        "      print(f\"\\nEarliest date: {earliest_date}\")\n",
        "      print(f\"Latest date: {latest_date}\")\n",
        "    else:\n",
        "       print(\"No valid date range found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfQxY10wsQmV",
        "outputId": "47470942-546e-4dd3-97a5-3af18f87ff3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Earliest date: 2012-08-15 20:01:36.621000\n",
            "Latest date: 2015-04-13 22:39:00.431000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task-2.\n",
        "\n",
        "#### Write a function which returns the year and month with the largest number of events."
      ],
      "metadata": {
        "id": "e8USMYuksVP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if busiest_year_month:\n",
        "    year, month = busiest_year_month\n",
        "    print(f\"\\nYear and month with the most events: {year}-{month:02}\")\n",
        "else:\n",
        "    print(\"No valid data for busiest year and month\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCoBA5r7sYWE",
        "outputId": "968e524b-cfe6-41ad-b190-2796af90e153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Year and month with the most events: 2015-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task-3.\n",
        "\n",
        "#### Describe what strategy you used to deal with the large size of this dataset\n"
      ],
      "metadata": {
        "id": "DzfvwGPm-VRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In order to properly handle the large dataset, I used chunk-based processing to manage memory and improve efficiency. By reading the file in chunks of 100,000 rows, I avoided overloading the system. Each chunk was cleaned by replacing null characters in the ACTIVITY_TIME column, removing missing values and duplicates, and standardizing the date format. Handling corrupted rows and encoding issues was challenging, but I overcame this by using on_bad_lines='skip' and specifying the latin-1 encoding. These steps allowed me to process the file without interruptions.\n",
        "\n",
        "I also calculated the overall date range and busiest year-month by aggregating results across chunks. Maintaining consistency in these calculations required careful integration of intermediate results. Despite occasional errors in individual chunks, the modular approach of isolating the cleaning and processing steps made debugging straightforward. This strategy ensured the dataset was processed efficiently, enabling me to extract key insights without compromising performance or accurac"
      ],
      "metadata": {
        "id": "3FKxIx-Pw1jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 : Standard Error of the Mean (SEM) with Bootstrapping\n",
        "\n"
      ],
      "metadata": {
        "id": "iOC2BgJKp25b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task-1.\n",
        "\n",
        "#### Calculate the SEM for age in the dataset."
      ],
      "metadata": {
        "id": "uypLd3P5svAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the local file path for the downloaded zip file\n",
        "local_zip_path = '/content/drive/MyDrive/BDA_Datasets/DATASETS/raw/hh_data_ml (1).zip'\n",
        "\n",
        "def calculate_age(row):\n",
        "    \"\"\"\n",
        "    Calculates the age of a person based on their birth month and year.\n",
        "    Assumes the current year is 2025.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): A row from a DataFrame containing birth year and month.\n",
        "\n",
        "    Returns:\n",
        "        int or None: Age of the person, or None if the data is invalid.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        birth_year = int(row['P07A'])\n",
        "        birth_month = int(row['P07M'])\n",
        "\n",
        "        if not (1 <= birth_month <= 12):  # Validate birth month\n",
        "            return None\n",
        "\n",
        "        age = 2025 - birth_year\n",
        "        return age\n",
        "    except (ValueError, TypeError):\n",
        "        return None\n",
        "\n",
        "def calculate_mean_age(data):\n",
        "    \"\"\"\n",
        "    Calculates the mean age, handling missing values.\n",
        "\n",
        "    Args:\n",
        "        data (pd.Series): A series of ages.\n",
        "\n",
        "    Returns:\n",
        "        float: The mean age, or NaN if no valid values are present.\n",
        "    \"\"\"\n",
        "    ages = data.dropna()\n",
        "    if ages.empty:\n",
        "        return np.nan\n",
        "    return ages.mean()\n",
        "\n",
        "def bootstrap_sample_mean(data, num_samples):\n",
        "    \"\"\"\n",
        "    Generates bootstrap samples and calculates their mean.\n",
        "\n",
        "    Args:\n",
        "        data (pd.Series): A series of ages.\n",
        "        num_samples (int): The number of bootstrap samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of mean values from bootstrap samples.\n",
        "    \"\"\"\n",
        "    means = []\n",
        "    for _ in range(num_samples):\n",
        "        sample = data.sample(frac=1, replace=True)\n",
        "        mean = calculate_mean_age(sample)\n",
        "        if not np.isnan(mean):\n",
        "            means.append(mean)\n",
        "    return means\n",
        "\n",
        "def calculate_sem(bootstrap_means):\n",
        "    \"\"\"\n",
        "    Calculates the Standard Error of the Mean (SEM) using bootstrap means.\n",
        "\n",
        "    Args:\n",
        "        bootstrap_means (list): A list of mean values from bootstrap samples.\n",
        "\n",
        "    Returns:\n",
        "        float or None: The SEM, or None if no bootstrap means are provided.\n",
        "    \"\"\"\n",
        "    if not bootstrap_means:\n",
        "        return None\n",
        "    return np.std(bootstrap_means, ddof=1)\n",
        "\n",
        "def process_data_chunked(local_zip_path, chunksize=10000, num_bootstrap_samples=100):\n",
        "    \"\"\"\n",
        "    Processes a large dataset in chunks to calculate the SEM of ages.\n",
        "\n",
        "    Args:\n",
        "        local_zip_path (str): Path to the zip file containing the dataset.\n",
        "        chunksize (int): Number of rows to process per chunk.\n",
        "        num_bootstrap_samples (int): Number of bootstrap samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        float or None: The SEM of ages, or None if processing fails.\n",
        "    \"\"\"\n",
        "    all_ages = []\n",
        "    total_chunks = 0  # Total number of chunks\n",
        "    processed_chunks = 0  # Number of successfully processed chunks\n",
        "\n",
        "    try:\n",
        "        with zipfile.ZipFile(local_zip_path, 'r') as z:\n",
        "            csv_file_name = z.namelist()[0]  # Get the first file in the zip\n",
        "            csv_bytes = z.read(csv_file_name)  # Read the CSV file as bytes\n",
        "            chunk_iterator = pd.read_csv(io.BytesIO(csv_bytes), chunksize=chunksize, sep='|')  # Load in chunks\n",
        "\n",
        "            for chunk in chunk_iterator:\n",
        "                total_chunks += 1\n",
        "                chunk.columns = chunk.columns.str.replace('|', '', regex=False)  # Clean column names\n",
        "                if 'P07M' in chunk.columns and 'P07A' in chunk.columns:\n",
        "                    chunk[['P07M', 'P07A']] = chunk[['P07M', 'P07A']].astype('int32')  # Convert columns to int32\n",
        "                    chunk['age'] = chunk.apply(calculate_age, axis=1)  # Calculate age\n",
        "                    all_ages.extend(chunk['age'].dropna())  # Append valid ages\n",
        "                    processed_chunks += 1\n",
        "                else:\n",
        "                    print(f\"Skipping chunk with columns: {chunk.columns}\")  # Log skipped chunks\n",
        "\n",
        "            if not all_ages:\n",
        "                return None\n",
        "            all_ages = pd.Series(all_ages)  # Convert list to a Series\n",
        "            bootstrap_means = bootstrap_sample_mean(all_ages, num_bootstrap_samples)  # Generate bootstrap samples\n",
        "            return calculate_sem(bootstrap_means)  # Calculate SEM\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the chunked data: {e}\")\n",
        "        return None\n",
        "\n",
        "# Inform the user about the estimated running time\n",
        "print(\"Estimated maximum running time: approximately 11 minutes.\")\n",
        "print(\"Please wait...\")\n",
        "\n",
        "# Load the data from the zip file and process it\n",
        "if local_zip_path:\n",
        "    num_bootstrap_samples = 100  # Define the number of bootstrap samples\n",
        "    start_time = time.time()  # Start time tracking\n",
        "    sem = process_data_chunked(local_zip_path, num_bootstrap_samples=num_bootstrap_samples)\n",
        "    end_time = time.time()  # End time tracking\n",
        "    if sem is not None:\n",
        "        print(f\"The Standard Error of the Mean (SEM) for age is: {sem:.4f}\")\n",
        "    else:\n",
        "        print(\"Could not calculate the SEM for age with the given data.\")\n",
        "else:\n",
        "    print(\"No valid file path found, cannot process.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNvht8KInaKk",
        "outputId": "eae9cb81-a696-43bc-9031-02ef05685db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated maximum running time: approximately 11 minutes.\n",
            "Please wait...\n",
            "The Standard Error of the Mean (SEM) for age is: 0.0041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task-2.\n",
        "\n",
        "#### Describe what strategy you used to deal with the fact that you have to run many computations for this task"
      ],
      "metadata": {
        "id": "SKNp27h7s5er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To address the computational challenges of processing a large dataset, I used a chunk-based approach, processing 10,000 rows at a time to optimize memory usage and prevent system overload. Each chunk was cleaned, transformed, and analyzed independently.\n",
        "\n",
        "Key challenges included managing memory constraints, handling missing or invalid data, and ensuring accurate statistical computations.\n",
        "\n",
        "To overcome these:\n",
        "\n",
        " - Memory Efficiency: By processing chunks iteratively and converting data types to memory-efficient formats (e.g., int32), I avoided excessive memory usage.\n",
        "\n",
        " - Data Quality: Missing or invalid birth month and year data were identified and excluded. Valid ages were calculated using custom functions and retained for analysis.\n",
        "\n",
        " - Robust Estimation: Bootstrapping (100 samples) was used to compute the Standard Error of the Mean (SEM), ensuring reliable results despite variability.\n",
        "\n",
        "The process included progress tracking and error handling to ensure smooth execution and quick debugging if issues arose. This strategy allowed for efficient computation, scaling well to large datasets while maintaining accuracy and reliability. The calculated SEM provided meaningful insights into the dataset within a reasonable runtime."
      ],
      "metadata": {
        "id": "016DP19dvR0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3 : Weather Forecast for All Capital Cities in Africa\n"
      ],
      "metadata": {
        "id": "oGLaT-xKsTWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task-1.\n",
        "#### Generate a CSV file with weather forecast for Monday, January 13, 2025"
      ],
      "metadata": {
        "id": "5AT-WNPwsnHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "file_path = '/content/drive/MyDrive/BDA_Datasets/Africa_Cities.csv'  # Path to the input CSV file containing city data\n",
        "df = pd.read_csv(file_path)  # Load the CSV file into a pandas DataFrame\n",
        "\n",
        "# Filter for African national capitals\n",
        "# Select rows where the city status matches specified categories and the continent is Africa\n",
        "african_capitals = df[\n",
        "    (df['STATUS'].isin(['National capital', 'National and provincial capital', 'Provincial capital', 'Other'])) &\n",
        "    (df['CONTINENT'] == 'Africa')\n",
        "][['CNTRY_NAME', 'CITY_NAME']].rename(columns={'CNTRY_NAME': 'Country', 'CITY_NAME': 'City'})\n",
        "\n",
        "# OpenWeather API configurations\n",
        "API_KEY = '0545df1a36bde6eb09fff3c6762420b1'  # API key for accessing OpenWeather services\n",
        "BASE_URL = 'https://api.openweathermap.org/data/2.5/forecast'  # Base URL for the OpenWeather API\n",
        "\n",
        "print(\"Estimated maximum running time: approximately 2 minutes.\")  # Inform user of estimated runtime\n",
        "print(\"Please wait...\")\n",
        "\n",
        "# Function to fetch weather data for a city\n",
        "def fetch_weather(city, country):\n",
        "    \"\"\"\n",
        "    Fetches weather forecast data for a given city and country.\n",
        "\n",
        "    Args:\n",
        "        city (str): The name of the city.\n",
        "        country (str): The name of the country.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: A dictionary containing weather data for January 13, 2025, or None if not found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Set up the API request parameters\n",
        "        params = {\n",
        "            'q': f'{city},{country}',\n",
        "            'appid': API_KEY,\n",
        "            'units': 'metric'  # Request temperature in Celsius\n",
        "        }\n",
        "        response = requests.get(BASE_URL, params=params)  # Make the API request\n",
        "        response.raise_for_status()  # Raise an error for unsuccessful responses\n",
        "        data = response.json()  # Parse the response JSON\n",
        "\n",
        "        # Extract weather forecast specifically for January 13, 2025\n",
        "        forecasts = data.get('list', [])  # Retrieve the list of forecasts\n",
        "        for forecast in forecasts:\n",
        "            # Convert Unix timestamp to a human-readable date\n",
        "            forecast_date = datetime.utcfromtimestamp(forecast['dt']).strftime('%Y-%m-%d')\n",
        "            if forecast_date == '2025-01-13':\n",
        "                # Extract relevant weather data\n",
        "                time = datetime.utcfromtimestamp(forecast['dt']).strftime('%H:%M:%S')\n",
        "                weather_main = forecast['weather'][0]['main']\n",
        "                temp = forecast['main']['temp']\n",
        "                temp_min = forecast['main']['temp_min']\n",
        "                temp_max = forecast['main']['temp_max']\n",
        "                humidity = forecast['main']['humidity']\n",
        "                clouds = forecast['clouds']['all']\n",
        "                return {\n",
        "                    'Date': '2025-01-13',\n",
        "                    'Time': time,\n",
        "                    'Weather_main': weather_main,\n",
        "                    'Temp': temp,\n",
        "                    'Temp_min': temp_min,\n",
        "                    'Temp_max': temp_max,\n",
        "                    'humidity': humidity,\n",
        "                    'Clouds': clouds\n",
        "                }\n",
        "        return None  # Return None if no data is found for the specified date\n",
        "    except Exception:\n",
        "        return None  # Skip processing cities with API or data issues\n",
        "\n",
        "# Fetch weather data for all African capitals\n",
        "weather_data = []  # List to store weather data for each city\n",
        "skipped_cities = []  # List to log cities with failed data retrieval\n",
        "\n",
        "for _, row in african_capitals.iterrows():\n",
        "    city, country = row['City'], row['Country']  # Extract city and country names\n",
        "    weather = fetch_weather(city, country)  # Fetch weather data for the city\n",
        "    if weather:\n",
        "        # Add successful weather data to the list\n",
        "        weather['Country'] = country\n",
        "        weather['City'] = city\n",
        "        weather_data.append(weather)\n",
        "    else:\n",
        "        # Log cities with failed data retrieval\n",
        "        skipped_cities.append({'City': city, 'Country': country})\n",
        "\n",
        "# Save the retrieved weather data to a CSV file\n",
        "output_df = pd.DataFrame(weather_data, columns=[\n",
        "    'Country', 'City', 'Date', 'Time', 'Weather_main', 'Temp', 'Temp_min', 'Temp_max', 'humidity', 'Clouds'\n",
        "])  # Create a DataFrame with specific columns\n",
        "output_file = '/content/african_capitals_weather.csv'  # Specify output file path\n",
        "output_df.to_csv(output_file, index=False)  # Save the DataFrame to a CSV file\n",
        "print(f\"Weather data saved to {output_file}\")  # Inform the user that the file has been saved\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UgoQVIeSvYT",
        "outputId": "2266d02a-ccd5-474e-fbe1-c194d466a564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated maximum running time: approximately 2 minutes.\n",
            "Please wait...\n",
            "Weather data saved to /content/african_capitals_weather.csv\n"
          ]
        }
      ]
    }
  ]
}
